torch<=2.8.0
transformers<=4.53.0
draccus<=0.11.5
accelerate
datasets>=4.0.0
peft
wandb
nvidia-ml-py
flash-linear-attention@git+https://github.com/jet-ai-projects/flash-linear-attention.git@jetai